{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(f\"{project_root}/src\")\n",
    "sys.path.append(f\"{project_root}/third_party\")\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_COLOR_PREFIX\"] = \"0\"\n",
    "\n",
    "\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "\n",
    "import ray.train\n",
    "import ray.train.torch\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import ray\n",
    "\n",
    "from document_processor import TextDocumentProcessor\n",
    "from token_processor import TikTokenizer\n",
    "from chunk_processor import ChunkProcessor\n",
    "from model.GPT import GPT\n",
    "from model.gpt_lr_scheduler import GPTLRScheduler\n",
    "from utility import resume_checkpoint, save_checkpoint\n",
    "from text_generator import TextGenerator\n",
    "from token_processor import TikTokenizer\n",
    "\n",
    "from config import gpt2_cfg as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(raylet) The autoscaler failed with the following error:\n",
      "Terminated with signal 15\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 709, in <module>\n",
      "    monitor.run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 584, in run\n",
      "    self._run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 438, in _run\n",
      "    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init(\n",
    "        runtime_env={\n",
    "            \"env_vars\": {          \n",
    "                \"PYTHONPATH\": \"$PYTHONPATH:\" + cfg.project_root + \"/src\",\n",
    "                \"RAY_DATA_VERBOSE_PROGRESS\": \"1\",\n",
    "            },\n",
    "            \"working_dir\": cfg.project_root,\n",
    "            \"excludes\": [\n",
    "                \"/bazel-*\",\n",
    "                \".git\",\n",
    "                \"*.pyc\",\n",
    "                \"/__pycache__\",\n",
    "                \"/outputs\",\n",
    "                \"/model\",\n",
    "            ],\n",
    "        },\n",
    "        ignore_reinit_error=True,\n",
    "    )\n",
    "\n",
    "# convience for debugging\n",
    "ray.data.DataContext.get_current().execution_options.verbose_progress = True\n",
    "ray.data.DataContext.log_internal_stack_trace_to_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_sources = [ Path(item[\"path\"]) for item in cfg[\"dataset\"]]\n",
    "text_document_paths = ray.data.from_items(data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import TextDocumentProcessor\n",
    "train_text_document_processor = TextDocumentProcessor(section=\"train\")\n",
    "train_texts=text_document_paths.map(train_text_document_processor)\n",
    "\n",
    "validate_text_document_processor = TextDocumentProcessor(section=\"validate\")\n",
    "validate_texts=text_document_paths.map(validate_text_document_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_processor import TikTokenizer\n",
    "tokenizer = TikTokenizer()\n",
    "train_tokens = train_texts.map(tokenizer)\n",
    "validate_tokens = validate_texts.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunk_processor import ChunkProcessor\n",
    "\n",
    "chunk_processor = ChunkProcessor(max_length=cfg[\"124M\"][\"block_size\"],stride=cfg[\"124M\"][\"stride\"])\n",
    "train_chunked_tokens = train_tokens.flat_map(chunk_processor)\n",
    "validate_chunked_tokens = validate_tokens.flat_map(chunk_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_chunked_tokens.materialize().stats())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _prepare_gradient_scaler(use_amp=True):\n",
    "    return torch.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "\n",
    "def _resume_training(best_checkpoint_dir, model, optimizer,scaler, device):\n",
    "    if os.path.exists(best_checkpoint_dir):\n",
    "        checkpoint = ray.train.Checkpoint.from_directory(best_checkpoint_dir)\n",
    "    else:\n",
    "        checkpoint = None\n",
    "    if checkpoint:\n",
    "        best_epoch, best_perplexity = resume_checkpoint(\n",
    "            model, optimizer, scaler, checkpoint,str(device)\n",
    "        )\n",
    "        epoch_start = best_epoch\n",
    "        print(f\"Resumed training from best_epoch {best_epoch},best_perplexity {best_perplexity}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found, starting from epoch 0\")\n",
    "    return epoch_start,best_epoch,best_perplexity\n",
    "\n",
    "\n",
    "def _prepare_metric(device):\n",
    "    metric = torchmetrics.text.Perplexity().to(device)\n",
    "    return metric\n",
    "\n",
    "\n",
    "def _prepare_loss_function():\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "def _prepare_lr_scheduler(warmup_steps, max_steps, max_lr, min_lr, optimizer):\n",
    "    scheduler = GPTLRScheduler(\n",
    "        optimizer,\n",
    "        warmup_steps=warmup_steps,\n",
    "        max_steps=max_steps,\n",
    "        max_lr=max_lr,\n",
    "        min_lr=min_lr,\n",
    "    )\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def _prepare_optimizer(weight_decay, max_lr, model, device):\n",
    "\n",
    "    # start with all of the candidate parameters\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    # filter out those that do not require grad\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "    print(\n",
    "        f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "    )\n",
    "    print(\n",
    "        f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "    )\n",
    "\n",
    "    # Create AdamW optimizer and use the fused version if it is available\n",
    "    fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and \"cuda\" in str(device)\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups,\n",
    "        lr=max_lr,\n",
    "        betas=(0.9, 0.95),\n",
    "        eps=1e-8,\n",
    "        **extra_args,\n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def _prepare_data():\n",
    "    train_data_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    validate_data_shard = ray.train.get_dataset_shard(\"validate\")\n",
    "    return train_data_shard, validate_data_shard\n",
    "\n",
    "\n",
    "def _prepare_model(\n",
    "    vocab_size,\n",
    "    dimension_embedding,\n",
    "    block_size,\n",
    "    num_layers,\n",
    "    num_headers,\n",
    "    drop_rate,\n",
    "    bias,\n",
    "):\n",
    "    model = GPT(\n",
    "        vocab_size,\n",
    "        dimension_embedding,\n",
    "        block_size,\n",
    "        num_layers,\n",
    "        num_headers,\n",
    "        drop_rate,\n",
    "        bias,\n",
    "    )\n",
    "    model = torch.compile(model)\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_workload_per_worker(cfg):\n",
    "    vocab_size = cfg[\"vocab_size\"]\n",
    "    dimension_embedding = cfg[\"dimension_embedding\"]\n",
    "    block_size = cfg[\"block_size\"]\n",
    "    num_layers = cfg[\"num_layers\"]\n",
    "    num_headers = cfg[\"num_headers\"]\n",
    "    drop_rate = cfg[\"drop_rate\"]\n",
    "    bias = cfg[\"bias\"]\n",
    "    check_frequency = cfg[\"check_frequency\"]\n",
    "    physical_training_batch_size_per_worker = cfg[\"physical_training_batch_size_per_worker\"]\n",
    "    physical_validate_batch_size_per_worker = cfg[\"physical_validate_batch_size_per_worker\"]\n",
    "    num_epoch_per_worker = cfg[\"num_epoch_per_worker\"]\n",
    "    resume_training = cfg[\"resume_training\"]\n",
    "    best_checkpoint_dir = cfg[\"best_checkpoint_dir\"]\n",
    "    warmup_steps = cfg[\"warmup_steps\"]\n",
    "    max_steps = cfg[\"max_steps\"]\n",
    "    max_lr = cfg[\"max_lr\"]\n",
    "    min_lr = cfg[\"min_lr\"]\n",
    "    weight_decay = cfg[\"weight_decay\"]\n",
    "    total_tokens_per_logical_batch_per_worker = cfg[\"total_tokens_per_logical_batch_per_worker\"]\n",
    "    data_type = cfg[\"data_type\"]\n",
    "\n",
    "    floating_point_precision = {\n",
    "        \"float32\": torch.float32,\n",
    "        \"bfloat16\": torch.bfloat16,\n",
    "        \"float16\": torch.float16,\n",
    "    }[data_type]\n",
    "\n",
    "    rank = ray.train.get_context().get_world_rank()\n",
    "    device =ray.train.torch.get_device()\n",
    "    use_amp = (floating_point_precision==\"float16\")\n",
    "\n",
    "    torch.manual_seed(1337 + rank)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    # data\n",
    "    train_data_shard, validate_data_shard = (\n",
    "        _prepare_data()\n",
    "    )\n",
    "\n",
    "    # GPT model\n",
    "    model = _prepare_model(\n",
    "        vocab_size,\n",
    "        dimension_embedding,\n",
    "        block_size,\n",
    "        num_layers,\n",
    "        num_headers,\n",
    "        drop_rate,\n",
    "        bias,\n",
    "    )\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = _prepare_optimizer(\n",
    "        weight_decay, max_lr, model, device\n",
    "    )\n",
    "\n",
    "    # initialize a GradScaler. Enable AMP for float16.\n",
    "    # According to the pytorch documentation:\n",
    "    #     \"When entering an autocast-enabled region, Tensors may be any type. \n",
    "    #     You should not call half() or bfloat16() on your model(s) or inputs \n",
    "    #     when using autocasting\"\n",
    "    scaler = _prepare_gradient_scaler(use_amp=use_amp)\n",
    "\n",
    "    # lr scheduler\n",
    "    scheduler = _prepare_lr_scheduler(\n",
    "        warmup_steps, max_steps, max_lr, min_lr, optimizer\n",
    "    )\n",
    "\n",
    "    # loss function\n",
    "    loss_function =_prepare_loss_function()\n",
    "\n",
    "    # metrics\n",
    "    metric = _prepare_metric(device)\n",
    "\n",
    "    # ====== Resume training state from the checkpoint. ======\n",
    "    epoch_start = 0\n",
    "    best_perplexity = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "\n",
    "    if resume_training:\n",
    "        epoch_start, best_epoch,best_perplexity  = (\n",
    "            _resume_training(best_checkpoint_dir, \n",
    "                                                            model, \n",
    "                                                            optimizer,\n",
    "                                                            scaler,\n",
    "                                                            device)\n",
    "            )\n",
    "        \n",
    "\n",
    "    report_metrics = {\n",
    "        \"rank\": rank,\n",
    "        \"epoch\": epoch_start,\n",
    "        \"token_total_\": 0,  # total tokens processed\n",
    "        \"token_process_time_ms\": 0.0, # time in ms\n",
    "        \"token_per_second\": 0.0,    # speed \n",
    "    \n",
    "        \"logical_train_loss\": 0.0,\n",
    "        \"logical_batch_count\": 0,\n",
    "    \n",
    "        \"norm\": 0.0,\n",
    "        \"validate_loss\": 0.0,\n",
    "        \"perplexity\": 0.0,\n",
    "        \n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_perplexity\": best_perplexity,\n",
    "    }\n",
    "\n",
    "    assert (\n",
    "        total_tokens_per_logical_batch_per_worker % (physical_training_batch_size_per_worker * block_size) == 0\n",
    "    ), \"total_batch_size must total_tokens_per_logical_batch_per_worker divisible by physical_training_batch_size_per_worker*block_size\"\n",
    "\n",
    "    logical_batch_size_per_worker = (\n",
    "        total_tokens_per_logical_batch_per_worker // block_size\n",
    "    )  # logical batch size\n",
    "\n",
    "    gradient_accumulation_steps = (\n",
    "        logical_batch_size_per_worker // physical_training_batch_size_per_worker\n",
    "    )\n",
    "\n",
    "    print(f\"total_tokens_per_logical_batch_per_worker: {total_tokens_per_logical_batch_per_worker}\")\n",
    "    print(f\"gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(epoch_start + 1, num_epoch_per_worker + 1):\n",
    "        token_processed = 0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        current_rank = ray.train.get_context().get_world_rank()\n",
    "        report_metrics[\"rank\"] = current_rank\n",
    "        report_metrics[\"epoch\"] = epoch\n",
    "\n",
    "        logical_train_loss = 0\n",
    "        logical_batch_count = 0\n",
    "        t0 = time.time()\n",
    "\n",
    "        for logical_batch in train_data_shard.iter_torch_batches(\n",
    "            batch_size=logical_batch_size_per_worker,\n",
    "            drop_last=False,\n",
    "            local_shuffle_buffer_size=1000,\n",
    "        ):\n",
    "            logical_batch_count += 1\n",
    "            logical_input_ids = logical_batch[\"input_ids\"]\n",
    "            logical_target_ids = logical_batch[\"target_ids\"]\n",
    "\n",
    "            \n",
    "            for step in range(gradient_accumulation_steps):\n",
    "                physical_input_ids_in_current_step = logical_input_ids[step : step + physical_training_batch_size_per_worker]\n",
    "                physical_target_ids_in_current_step = logical_target_ids[step : step + physical_training_batch_size_per_worker]\n",
    "                \n",
    "                # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n",
    "                with torch.autocast(device_type=device.type,dtype=floating_point_precision,enabled=use_amp,):\n",
    "                    logits = model(physical_input_ids_in_current_step)\n",
    "                    physical_loss = loss_function(logits.flatten(0, 1),physical_target_ids_in_current_step.flatten(),)\n",
    "                    physical_loss = (physical_loss / gradient_accumulation_steps)  # normalize the loss to account for the gradient accumulation\n",
    "\n",
    "                \n",
    "                # require_backward_grad_sync is set to True for the last step in the gradient accumulation  \n",
    "                # to speed up the training process by reducing the synchronization overhead across workers.\n",
    "                model.require_backward_grad_sync = (step == gradient_accumulation_steps - 1)\n",
    "                \n",
    "                # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "                # Backward passes under autocast are not recommended.\n",
    "                # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "                scaler.scale(physical_loss).backward()\n",
    "\n",
    "                # for reporting\n",
    "                logical_train_loss += (physical_loss.detach().item())\n",
    "                token_processed += (physical_training_batch_size_per_worker * block_size)\n",
    "\n",
    "            # Unscales the gradients of optimizer's assigned parameters in-place for clipping.\n",
    "            scaler.unscale_(optimizer)\n",
    "            norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "            # otherwise, optimizer.step() is skipped\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        assert logical_batch_count > 0, \"logical_batch_count must be greater than 0\"\n",
    "\n",
    "        logical_train_loss = logical_train_loss / logical_batch_count\n",
    "        report_metrics[\"logical_train_loss\"] = logical_train_loss\n",
    "        report_metrics[\"logical_batch_count\"] = logical_batch_count\n",
    "\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "\n",
    "        token_per_second = token_processed / dt\n",
    "\n",
    "        \n",
    "        report_metrics[\"token_total\"] = token_processed\n",
    "        report_metrics[\"token_process_time_ms\"] = dt * 1000\n",
    "        report_metrics[\"token_per_second\"] = token_per_second\n",
    "\n",
    "        report_metrics[\"norm\"] = norm.item()\n",
    "\n",
    "        # Evaluate the model on the validation set only if the check_frequency is met \n",
    "        # and the current worker is the rank 0 worker\n",
    "        if epoch % check_frequency == 0 and ray.train.get_context().get_world_rank() == 0:\n",
    "            validate_loss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                validate_batch_count = 0\n",
    "                for batch in validate_data_shard.iter_torch_batches(batch_size=physical_validate_batch_size_per_worker,drop_last=False,):\n",
    "                    validate_batch_count += 1\n",
    "                    input_ids = batch[\"input_ids\"]\n",
    "                    target_ids = batch[\"target_ids\"]\n",
    "\n",
    "                    with torch.autocast(device_type=device.type,dtype=floating_point_precision,enabled=use_amp,):\n",
    "                        logits = model(input_ids)\n",
    "                        loss = loss_function(logits.flatten(0, 1), target_ids.flatten())\n",
    "\n",
    "                    validate_loss += loss.item()  # only for reporting\n",
    "                    metric.update(logits, target_ids)\n",
    "\n",
    "            validate_loss = validate_loss / validate_batch_count\n",
    "            perplexity = metric.compute().item()\n",
    "            metric.reset()\n",
    "\n",
    "            report_metrics[\"validate_loss\"] = validate_loss\n",
    "            report_metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "            if perplexity < best_perplexity:\n",
    "                best_perplexity = perplexity\n",
    "                best_epoch = epoch\n",
    "\n",
    "                report_metrics[\"best_epoch\"] = best_epoch\n",
    "                report_metrics[\"best_perplexity\"] = best_perplexity\n",
    "\n",
    "                # In standard DDP training, where the model is the same across all ranks,\n",
    "                # so only the global rank 0 worker needs to save and report the checkpoint\n",
    "                \n",
    "                # create the best_checkpoint_dir if it does not exist\n",
    "                if not os.path.exists(best_checkpoint_dir):\n",
    "                    os.makedirs(best_checkpoint_dir)\n",
    "\n",
    "                save_checkpoint(\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    scaler,\n",
    "                    epoch,\n",
    "                    perplexity,\n",
    "                    best_checkpoint_dir,\n",
    "                )\n",
    "\n",
    "        ray.train.report(metrics=report_metrics)\n",
    "\n",
    "        # Create a TextGenerator instance\n",
    "        # start_context = \"To be or not to be\"\n",
    "        # text_generator = TextGenerator(model, device =ray.train.torch.get_device())\n",
    "        # encoded_tensor = tokenizer.encode(start_context)\n",
    "        \n",
    "        # # Generate new text\n",
    "        # decoded = text_generator(encoded_tensor, max_new_tokens=50, block_size=1024)\n",
    "\n",
    "        # print(f\"\\n epoch-{epoch}:{decoded}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loop_config = {\n",
    "    \"vocab_size\": cfg[\"124M\"][\"vocab_size\"],\n",
    "    \"dimension_embedding\": cfg[\"124M\"][\"dimension_embedding\"],\n",
    "    \"block_size\": cfg[\"124M\"][\"block_size\"],\n",
    "    \"num_layers\": cfg[\"124M\"][\"num_layers\"],\n",
    "    \"num_headers\": cfg[\"124M\"][\"num_headers\"],\n",
    "    \"drop_rate\": cfg[\"124M\"][\"drop_rate\"],\n",
    "    \"bias\": cfg[\"124M\"][\"bias\"],\n",
    "    \"check_frequency\": cfg[\"ray_train\"][\"check_frequency\"],\n",
    "    \"physical_training_batch_size_per_worker\": cfg[\"ray_train\"][\n",
    "        \"physical_training_batch_size_per_worker\"\n",
    "    ],\n",
    "    \"physical_validate_batch_size_per_worker\": cfg[\"ray_train\"][\n",
    "        \"physical_validate_batch_size_per_worker\"\n",
    "    ],\n",
    "    \"num_epoch_per_worker\": cfg[\"ray_train\"][\"num_epoch_per_worker\"],\n",
    "    \"resume_training\": cfg[\"ray_train\"][\"resume_training\"],\n",
    "    \"best_checkpoint_dir\": cfg[\"ray_train\"][\"best_checkpoint_dir\"],\n",
    "    \"start_context\": cfg[\"ray_train\"][\"start_context\"],\n",
    "    \"warmup_steps\": cfg[\"ray_train\"][\"warmup_steps\"],\n",
    "    \"max_steps\": cfg[\"ray_train\"][\"max_steps\"],\n",
    "    \"max_lr\": cfg[\"ray_train\"][\"max_lr\"],\n",
    "    \"min_lr\": cfg[\"ray_train\"][\"min_lr\"],\n",
    "    \"weight_decay\": cfg[\"ray_train\"][\"weight_decay\"],\n",
    "    \"total_tokens_per_logical_batch_per_worker\": cfg[\"ray_train\"][\n",
    "        \"total_tokens_per_logical_batch_per_worker\"\n",
    "    ],\n",
    "    \"data_type\": cfg[\"ray_train\"][\"data_type\"],\n",
    "}\n",
    "\n",
    "dataset = cfg[\"dataset\"]\n",
    "block_size = cfg[\"124M\"][\"block_size\"]\n",
    "stride = cfg[\"124M\"][\"stride\"]\n",
    "\n",
    "train_chunked_tokens, validate_chunked_tokens = (\n",
    "    train_chunked_tokens,\n",
    "    validate_chunked_tokens,\n",
    ")\n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_workload_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    datasets={\n",
    "        \"train\": train_chunked_tokens,\n",
    "        \"validate\": validate_chunked_tokens,\n",
    "    },\n",
    "    dataset_config=ray.train.DataConfig(\n",
    "        datasets_to_split=[\"train\"],  # only split the train dataset into shards\n",
    "    ),\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=cfg[\"ray_train\"][\"num_workers\"],\n",
    "        use_gpu=cfg[\"ray_train\"][\"use_gpu\"],\n",
    "        resources_per_worker={\n",
    "            \"CPU\": cfg[\"ray_train\"][\"num_cpus_per_worker\"],\n",
    "            \"GPU\": cfg[\"ray_train\"][\"num_gpus_per_worker\"],\n",
    "        },\n",
    "    ),\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=cfg[\"ray_train\"][\"storage_path\"],\n",
    "        name=cfg[\"ray_train\"][\"name\"],\n",
    "    ),\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
