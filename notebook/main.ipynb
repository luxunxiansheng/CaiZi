{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(f\"{project_root}/src\")\n",
    "sys.path.append(f\"{project_root}/third_party\")\n",
    "\n",
    "from config import gpt2_cfg as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    ray.init(\n",
    "        runtime_env={\n",
    "            \"env_vars\": {          \n",
    "                \"PYTHONPATH\": \"$PYTHONPATH:\" + cfg.project_root + \"/src\",\n",
    "            },\n",
    "            \"working_dir\": cfg.project_root,\n",
    "            \"excludes\": [\n",
    "                \"/bazel-*\",\n",
    "                \".git\",\n",
    "                \"*.pyc\",\n",
    "                \"/__pycache__\",\n",
    "                \"/outputs\",\n",
    "                \"/model\",\n",
    "            ],\n",
    "        },\n",
    "        _metrics_export_port=8080,\n",
    "    )\n",
    "# convience for debugging\n",
    "ray.data.DataContext.get_current().execution_options.verbose_progress = True\n",
    "ray.data.DataContext.log_internal_stack_trace_to_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_sources = [ Path(item[\"path\"]) for item in cfg[\"dataset\"]]\n",
    "text_document_paths = ray.data.from_items(data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import TextDocumentProcessor\n",
    "train_text_document_processor = TextDocumentProcessor(section=\"train\")\n",
    "train_texts=text_document_paths.map(train_text_document_processor)\n",
    "\n",
    "validate_text_document_processor = TextDocumentProcessor(section=\"validate\")\n",
    "validate_texts=text_document_paths.map(validate_text_document_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_processor import TikTokenizer\n",
    "tokenizer = TikTokenizer()\n",
    "train_tokens = train_texts.map(tokenizer)\n",
    "validate_tokens = validate_texts.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunk_processor import ChunkProcessor\n",
    "chunk_processor = ChunkProcessor()\n",
    "train_chunked_tokens = train_tokens.flat_map(chunk_processor)\n",
    "validate_chunked_tokens = validate_tokens.flat_map(chunk_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torcheval.metrics.text import Perplexity\n",
    "\n",
    "import ray\n",
    "from model.GPT import GPT\n",
    "from utility import save_checkpoint, resume_checkpoint\n",
    "\n",
    "\n",
    "\n",
    "train_loop_config = {\n",
    "    \"vocab_size\": cfg[\"124M\"][\"vocab_size\"],\n",
    "    \"dimension_embedding\": cfg[\"124M\"][\"dimension_embedding\"],\n",
    "    \"block_size\": cfg[\"124M\"][\"block_size\"],\n",
    "    \"n_layers\": cfg[\"124M\"][\"n_layers\"],\n",
    "    \"num_header\": cfg[\"124M\"][\"num_header\"],\n",
    "    \"drop_rate\": cfg[\"124M\"][\"drop_rate\"],\n",
    "    \"qkv_bias\": cfg[\"124M\"][\"qkv_bias\"],\n",
    "    \"check_frequency\": cfg[\"ray_train\"][\"check_frequency\"],\n",
    "    \"batch_size_per_worker\": cfg[\"ray_train\"][\"batch_size_per_worker\"],\n",
    "    \"epoch_start\": cfg[\"ray_train\"][\"epoch_start\"],\n",
    "    \"num_epoch_per_worker\": cfg[\"ray_train\"][\"num_epoch_per_worker\"]\n",
    "}\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    vocab_size=config[\"vocab_size\"] \n",
    "    dimension_embedding=config[\"dimension_embedding\"]\n",
    "    block_size=config[\"block_size\"]\n",
    "    n_layers=config[\"n_layers\"]\n",
    "    num_header=config[\"num_header\"] \n",
    "    drop_rate=config[\"drop_rate\"]\n",
    "    qkv_bias=config[\"qkv_bias\"]\n",
    "    check_frequency = config[\"check_frequency\"]\n",
    "    batch_size_per_worker=config[\"batch_size_per_worker\"]\n",
    "    epoch_start=config[\"epoch_start\"]\n",
    "    num_epoch_per_worker=config[\"num_epoch_per_worker\"]\n",
    "    \n",
    "    \n",
    "    # GPT model \n",
    "    model = GPT(vocab_size, dimension_embedding, block_size,n_layers, num_header, drop_rate, qkv_bias)\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "    \n",
    "    # ====== Resume training state from the checkpoint. ======\n",
    "    epoch_start = 0\n",
    "    \n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "       epoch_start = resume_checkpoint(model, optimizer, checkpoint)\n",
    "    \n",
    "    \n",
    "    # loss function\n",
    "    loss_function =  torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # metrics    \n",
    "    metric=Perplexity()\n",
    "    \n",
    "    # data \n",
    "    train_data_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    train_dataloader = train_data_shard.iter_torch_batches(batch_size=batch_size_per_worker)\n",
    "    validate_data_shard   = ray.train.get_dataset_shard(\"validate\")\n",
    "    validate_dataloader = validate_data_shard.iter_torch_batches(batch_size=1)\n",
    "\n",
    "    for epoch in range(epoch_start, num_epoch_per_worker):\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            outputs = model(input_ids)\n",
    "            target_ids = batch[\"target_ids\"]\n",
    "            loss = loss_function(outputs.flatten(), target_ids.flatten())\n",
    "            train_loss = loss.item() # only for reporting\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "                   \n",
    "        validate_loss = 0\n",
    "        perplexity = 0\n",
    "        checkpoint = None\n",
    "  \n",
    "        if epoch % check_frequency == 0:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():                \n",
    "                for batch in validate_dataloader:\n",
    "                    input_ids = batch[\"input_ids\"]\n",
    "                    outputs = model(input_ids)\n",
    "                    target_ids = batch[\"target_ids\"]                    \n",
    "                    loss = loss_function(outputs.flatten(), target_ids.flatten())  \n",
    "                    validate_loss = loss.item()  # only for reporting\n",
    "                    metric.update(outputs, target_ids)       \n",
    "                perplexity= metric.compute().item()\n",
    "            \n",
    "            metric.reset()  \n",
    "           \n",
    "            with tempfile.TemporaryDirectory() as temp_checkpoint_dir:                \n",
    "                # In standard DDP training, where the model is the same across all ranks,\n",
    "                # only the global rank 0 worker needs to save and report the checkpoint\n",
    "                if ray.train.get_context().get_world_rank() == 0:\n",
    "                        # === Make sure to save all state needed for resuming training ===\n",
    "                        save_checkpoint(model, optimizer, epoch, temp_checkpoint_dir)\n",
    "                \n",
    "\n",
    "            ray.train.report(\n",
    "                metrics= {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"validate_loss\": validate_loss,\n",
    "                        \"perplexity\": perplexity,\n",
    "                    },\n",
    "                checkpoint=checkpoint\n",
    "                ) \n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config= train_loop_config,\n",
    "    datasets={\n",
    "        \"train\": train_chunked_tokens,\n",
    "        \"validate\": validate_chunked_tokens,\n",
    "    },\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=cfg[\"ray_train\"][\"num_workers\"],\n",
    "        use_gpu=    cfg[\"ray_train\"][\"use_gpu\"],\n",
    "        resources_per_worker={\"CPU\": cfg[\"ray_train\"][\"num_cpus_per_worker\"], \"GPU\": cfg[\"ray_train\"][\"num_gpus_per_worker\"]},\n",
    "    ),\n",
    "\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
