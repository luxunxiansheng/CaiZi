{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(f\"{project_root}/src\")\n",
    "sys.path.append(f\"{project_root}/third_party\")\n",
    "\n",
    "from config import gpt2_cfg as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 07:54:41,100\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-08-02 07:54:41,106\tINFO packaging.py:530 -- Creating a file package for local directory '/workspaces/CaiZi'.\n",
      "2024-08-02 07:54:41,111\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_60aaa3dbfbbaea59.zip' (0.32MiB) to Ray cluster...\n",
      "2024-08-02 07:54:41,113\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_60aaa3dbfbbaea59.zip'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "\n",
    "ray.init(\n",
    "        runtime_env={\n",
    "            \"env_vars\": {          \n",
    "                \"PYTHONPATH\": \"$PYTHONPATH:\" + cfg.project_root + \"/src\",\n",
    "            },\n",
    "            \"working_dir\": cfg.project_root,\n",
    "            \"excludes\": [\n",
    "                \"/bazel-*\",\n",
    "                \".git\",\n",
    "                \"*.pyc\",\n",
    "                \"/__pycache__\",\n",
    "                \"/outputs\",\n",
    "                \"/model\",\n",
    "            ],\n",
    "        },\n",
    "      \n",
    "    )\n",
    "# convience for debugging\n",
    "ray.data.DataContext.get_current().execution_options.verbose_progress = True\n",
    "ray.data.DataContext.log_internal_stack_trace_to_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_sources = [ Path(item[\"path\"]) for item in cfg[\"dataset\"]]\n",
    "text_document_paths = ray.data.from_items(data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import TextDocumentProcessor\n",
    "train_text_document_processor = TextDocumentProcessor(section=\"train\")\n",
    "train_texts=text_document_paths.map(train_text_document_processor)\n",
    "\n",
    "validate_text_document_processor = TextDocumentProcessor(section=\"validate\")\n",
    "validate_texts=text_document_paths.map(validate_text_document_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_processor import TikTokenizer\n",
    "tokenizer = TikTokenizer()\n",
    "train_tokens = train_texts.map(tokenizer)\n",
    "validate_tokens = validate_texts.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunk_processor import ChunkProcessor\n",
    "\n",
    "chunk_processor = ChunkProcessor(max_length=cfg[\"124M\"][\"block_size\"],stride=cfg[\"124M\"][\"stride\"])\n",
    "train_chunked_tokens = train_tokens.flat_map(chunk_processor)\n",
    "validate_chunked_tokens = validate_tokens.flat_map(chunk_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "\n",
    "import ray.train\n",
    "import torch\n",
    "from torchmetrics.text import Perplexity\n",
    "\n",
    "import ray\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "from model.GPT import GPT\n",
    "from utility import save_checkpoint, resume_checkpoint\n",
    "\n",
    "train_loop_config = {\n",
    "    \"vocab_size\": cfg[\"124M\"][\"vocab_size\"],\n",
    "    \"dimension_embedding\": cfg[\"124M\"][\"dimension_embedding\"],\n",
    "    \"block_size\": cfg[\"124M\"][\"block_size\"],\n",
    "    \"num_layers\": cfg[\"124M\"][\"num_layers\"],\n",
    "    \"num_headers\": cfg[\"124M\"][\"num_headers\"],\n",
    "    \"drop_rate\": cfg[\"124M\"][\"drop_rate\"],\n",
    "    \"qkv_bias\": cfg[\"124M\"][\"qkv_bias\"],\n",
    "    \"check_frequency\": cfg[\"ray_train\"][\"check_frequency\"],\n",
    "    \"batch_size_per_worker\": cfg[\"ray_train\"][\"batch_size_per_worker\"],\n",
    "    \"num_epoch_per_worker\": cfg[\"ray_train\"][\"num_epoch_per_worker\"],\n",
    "}\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    dimension_embedding = config[\"dimension_embedding\"]\n",
    "    block_size = config[\"block_size\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    num_headers = config[\"num_headers\"]\n",
    "    drop_rate = config[\"drop_rate\"]\n",
    "    qkv_bias = config[\"qkv_bias\"]\n",
    "    check_frequency = config[\"check_frequency\"]\n",
    "    batch_size_per_worker = config[\"batch_size_per_worker\"]\n",
    "    num_epoch_per_worker = config[\"num_epoch_per_worker\"]\n",
    "\n",
    "    # GPT model\n",
    "    model = GPT(\n",
    "        vocab_size,\n",
    "        dimension_embedding,\n",
    "        block_size,\n",
    "        num_layers,\n",
    "        num_headers,\n",
    "        drop_rate,\n",
    "        qkv_bias,\n",
    "    )\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "    # ====== Resume training state from the checkpoint. ======\n",
    "    epoch_start = 0\n",
    "\n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        epoch_start = resume_checkpoint(model, optimizer, checkpoint)\n",
    "\n",
    "    # loss function\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    rank = ray.train.get_context().get_world_rank()\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # metrics\n",
    "    metric = Perplexity().to(device)\n",
    "\n",
    "    # data\n",
    "    train_data_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    validate_data_shard = ray.train.get_dataset_shard(\"validate\")\n",
    "\n",
    "    for epoch in range(epoch_start, num_epoch_per_worker):\n",
    "        model.train()\n",
    "        \n",
    "        report_metrics = {}\n",
    "\n",
    "        train_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch in train_data_shard.iter_torch_batches(\n",
    "            batch_size=batch_size_per_worker,\n",
    "            drop_last=True,\n",
    "            local_shuffle_buffer_size=1000,\n",
    "        ):\n",
    "            batch_count += 1\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            logits = model(input_ids)\n",
    "            target_ids = batch[\"target_ids\"]\n",
    "            loss = loss_function(logits.flatten(0, 1), target_ids.flatten())\n",
    "            train_loss += loss.item()  # only for reporting\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / batch_count\n",
    "        \n",
    "        report_metrics[\"epoch\"] = epoch\n",
    "        report_metrics[\"train_loss\"] = train_loss\n",
    "        \n",
    "\n",
    "        validate_loss = 0\n",
    "        perplexity = 0\n",
    "        checkpoint = None\n",
    "\n",
    "        if epoch % check_frequency == 0:\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_count = 0\n",
    "                for batch in validate_data_shard.iter_torch_batches(\n",
    "                    batch_size=1,\n",
    "                    drop_last=False,\n",
    "                ):\n",
    "                    batch_count += 1\n",
    "                    input_ids = batch[\"input_ids\"]\n",
    "                    logits = model(input_ids)\n",
    "                    target_ids = batch[\"target_ids\"]\n",
    "                    loss = loss_function(logits.flatten(0, 1), target_ids.flatten())\n",
    "                    validate_loss += loss.item()  # only for reporting\n",
    "                    metric.update(logits, target_ids)\n",
    "\n",
    "            validate_loss = validate_loss / batch_count\n",
    "            perplexity = metric.compute().item()\n",
    "\n",
    "            metric.reset()\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "                # In standard DDP training, where the model is the same across all ranks,\n",
    "                # only the global rank 0 worker needs to save and report the checkpoint\n",
    "                if ray.train.get_context().get_world_rank() == 0:\n",
    "                    # === Make sure to save all state needed for resuming training ===\n",
    "                    save_checkpoint(model, optimizer, epoch, temp_checkpoint_dir)\n",
    "                    checkpoint =  Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "            report_metrics[\"validate_loss\"] = validate_loss\n",
    "            report_metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "        ray.train.report(\n",
    "            metrics=report_metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 07:54:44,570\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-08-02 07:54:44 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-08-02_07-54-40_383230_177124/artifacts/2024-08-02_07-54-44/TorchTrainer_2024-08-02_07-54-44/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-08-02 07:54:49 (running for 00:00:05.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 2.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-08-02_07-54-40_383230_177124/artifacts/2024-08-02_07-54-44/TorchTrainer_2024-08-02_07-54-44/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=179889)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=179889)\u001b[0m [W802 07:54:48.548534460 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "\u001b[36m(TorchTrainer pid=179532)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=179532)\u001b[0m - (node_id=7549af926058939bed624d7e42c52394a05c512b39008387cca95149, ip=172.17.0.2, pid=179889) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=179889)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(SplitCoordinator pid=179965)\u001b[0m Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-08-02_07-54-40_383230_177124/logs/ray-data\n",
      "\u001b[36m(SplitCoordinator pid=179965)\u001b[0m Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(TextDocumentProcessor)->Map(TikTokenizer)->FlatMap(ChunkProcessor)] -> OutputSplitter[split(1, equal=True)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24f80f8be1343fd9576554cca5e3023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=179965) - Map(TextDocumentProcessor)->Map(TikTokenizer)->FlatMap(ChunkProcessor) 1: 0 bundle [00:00, ? bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edb3045f7de4afa945ee649ec045820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=179965) - split(1, equal=True) 2: 0 bundle [00:00, ? bundle/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef7759ef647428a84211ffd4082eb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=179965) Running 0: 0 bundle [00:00, ? bundle/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d10ea7645964ce19733cad8a04dfa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=179966) - Map(TextDocumentProcessor)->Map(TikTokenizer)->FlatMap(ChunkProcessor) 1: 0 bundle [00:00, ? bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd51b51028884bc9ad60f73f3e1cd696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=179966) - split(1, equal=True) 2: 0 bundle [00:00, ? bundle/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e75cef9d1be4a14bccc3776f4b2e769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=179966) Running 0: 0 bundle [00:00, ? bundle/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 07:54:53,583\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_7b53e_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2659, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=179532, ip=172.17.0.2, actor_id=1ccdb23e26fb750d67da379001000000, repr=TorchTrainer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=179889, ip=172.17.0.2, actor_id=2a04bf15ae3db9442f15d58901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x78610eb534c0>)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_177124/3569920317.py\", line 142, in train_loop_per_worker\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 658, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 749, in report\n",
      "    _get_session().report(metrics, checkpoint=checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 427, in report\n",
      "    persisted_checkpoint = self.storage.persist_current_checkpoint(checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 545, in persist_current_checkpoint\n",
      "    _pyarrow_fs_copy_files(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 110, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyarrow/fs.py\", line 263, in copy_files\n",
      "    _copy_files(source_fs, source_path,\n",
      "  File \"pyarrow/_fs.pyx\", line 1611, in pyarrow._fs._copy_files\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Failed to open local file '/tmp/tmpr9nrgqxf'. Detail: [errno 2] No such file or directory\n",
      "2024-08-02 07:54:53,587\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/TorchTrainer_2024-08-02_07-54-44' in 0.0016s.\n",
      "2024-08-02 07:54:53,589\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_7b53e_00000]\n",
      "2024-08-02 07:54:53,589\tINFO tune.py:1041 -- Total run time: 9.02 seconds (9.01 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-08-02 07:54:53 (running for 00:00:09.01)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 2.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-08-02_07-54-40_383230_177124/artifacts/2024-08-02_07-54-44/TorchTrainer_2024-08-02_07-54-44/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "Number of errored trials: 1\n",
      "+--------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name               |   # failures | error file                                                                                                                                                                                  |\n",
      "|--------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| TorchTrainer_7b53e_00000 |            1 | /tmp/ray/session_2024-08-02_07-54-40_383230_177124/artifacts/2024-08-02_07-54-44/TorchTrainer_2024-08-02_07-54-44/driver_artifacts/TorchTrainer_7b53e_00000_0_2024-08-02_07-54-44/error.txt |\n",
      "+--------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/root/ray_results/TorchTrainer_2024-08-02_07-54-44\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(FileNotFoundError)\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;31mRayTaskError(FileNotFoundError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=179532, ip=172.17.0.2, actor_id=1ccdb23e26fb750d67da379001000000, repr=TorchTrainer)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=179889, ip=172.17.0.2, actor_id=2a04bf15ae3db9442f15d58901000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x78610eb534c0>)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"/tmp/ipykernel_177124/3569920317.py\", line 142, in train_loop_per_worker\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 658, in wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 749, in report\n    _get_session().report(metrics, checkpoint=checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/session.py\", line 427, in report\n    persisted_checkpoint = self.storage.persist_current_checkpoint(checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 545, in persist_current_checkpoint\n    _pyarrow_fs_copy_files(\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/storage.py\", line 110, in _pyarrow_fs_copy_files\n    return pyarrow.fs.copy_files(\n  File \"/opt/conda/lib/python3.10/site-packages/pyarrow/fs.py\", line 263, in copy_files\n    _copy_files(source_fs, source_path,\n  File \"pyarrow/_fs.pyx\", line 1611, in pyarrow._fs._copy_files\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Failed to open local file '/tmp/tmpr9nrgqxf'. Detail: [errno 2] No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Result\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TorchTrainer(\n\u001b[1;32m      4\u001b[0m     train_loop_per_worker\u001b[38;5;241m=\u001b[39mtrain_loop_per_worker,\n\u001b[1;32m      5\u001b[0m     train_loop_config\u001b[38;5;241m=\u001b[39m train_loop_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m result:Result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py:638\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([restore_msg, TrainingFailedError\u001b[38;5;241m.\u001b[39m_FAILURE_CONFIG_MSG])\n\u001b[1;32m    640\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresult\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/root/ray_results/TorchTrainer_2024-08-02_07-54-44\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "from ray.train.torch import  TorchTrainer\n",
    "from ray.train import Result\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config= train_loop_config,\n",
    "    datasets={\n",
    "        \"train\": train_chunked_tokens,\n",
    "        \"validate\": validate_chunked_tokens,\n",
    "    },\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=cfg[\"ray_train\"][\"num_workers\"],\n",
    "        use_gpu=    cfg[\"ray_train\"][\"use_gpu\"],\n",
    "        resources_per_worker={\"CPU\": cfg[\"ray_train\"][\"num_cpus_per_worker\"], \"GPU\": cfg[\"ray_train\"][\"num_gpus_per_worker\"]},\n",
    "    ),\n",
    "\n",
    ")\n",
    "result:Result = trainer.fit()\n",
    "print(result.metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
