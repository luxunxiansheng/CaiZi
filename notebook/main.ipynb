{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(f\"{project_root}/src\")\n",
    "sys.path.append(f\"{project_root}/third_party\")\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "os.environ[\"RAY_COLOR_PREFIX\"] = \"0\"\n",
    "\n",
    "from config import gpt2_cfg as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init(\n",
    "        runtime_env={\n",
    "            \"env_vars\": {          \n",
    "                \"PYTHONPATH\": \"$PYTHONPATH:\" + cfg.project_root + \"/src\",\n",
    "                \"RAY_DATA_VERBOSE_PROGRESS\": \"1\",\n",
    "            },\n",
    "            \"working_dir\": cfg.project_root,\n",
    "            \"excludes\": [\n",
    "                \"/bazel-*\",\n",
    "                \".git\",\n",
    "                \"*.pyc\",\n",
    "                \"/__pycache__\",\n",
    "                \"/outputs\",\n",
    "                \"/model\",\n",
    "            ],\n",
    "        },\n",
    "        ignore_reinit_error=True,\n",
    "    )\n",
    "\n",
    "# convience for debugging\n",
    "ray.data.DataContext.get_current().execution_options.verbose_progress = False\n",
    "ray.data.DataContext.log_internal_stack_trace_to_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_sources = [ Path(item[\"path\"]) for item in cfg[\"dataset\"]]\n",
    "text_document_paths = ray.data.from_items(data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import TextDocumentProcessor\n",
    "train_text_document_processor = TextDocumentProcessor(section=\"train\")\n",
    "train_texts=text_document_paths.map(train_text_document_processor)\n",
    "\n",
    "validate_text_document_processor = TextDocumentProcessor(section=\"validate\")\n",
    "validate_texts=text_document_paths.map(validate_text_document_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_processor import TikTokenizer\n",
    "tokenizer = TikTokenizer()\n",
    "train_tokens = train_texts.map(tokenizer)\n",
    "validate_tokens = validate_texts.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunk_processor import ChunkProcessor\n",
    "\n",
    "chunk_processor = ChunkProcessor(max_length=cfg[\"124M\"][\"block_size\"],stride=cfg[\"124M\"][\"stride\"])\n",
    "train_chunked_tokens = train_tokens.flat_map(chunk_processor)\n",
    "validate_chunked_tokens = validate_tokens.flat_map(chunk_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:26:18,552\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-08-07_15-09-39_319166_12085/logs/ray-data\n",
      "2024-08-07 15:26:18,554\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(TextDocumentProcessor)->Map(TikTokenizer)->FlatMap(ChunkProcessor)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bec0c00345f441197481d0f10904cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0 bundle [00:00, ? bundle/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:26:19,364\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-08-07_15-09-39_319166_12085/logs/ray-data\n",
      "2024-08-07 15:26:19,367\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(TextDocumentProcessor)->Map(TikTokenizer)->FlatMap(ChunkProcessor)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5962, 22307,    25,  ...,  1842,   484,  6842],\n",
      "        [  514,    13,   198,  ...,   275,  1000,    13]], device='cuda:0'), 'target_ids': tensor([[22307,    25,   198,  ...,   484,  6842,   514],\n",
      "        [   13,   198,   198,  ...,  1000,    13,   198]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fd0bb8e84b487491054c0a0bc21503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0 bundle [00:00, ? bundle/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5962, 22307,    25,  ...,  1842,   484,  6842],\n",
      "        [  514,    13,   198,  ...,   275,  1000,    13]], device='cuda:0'), 'target_ids': tensor([[22307,    25,   198,  ...,   484,  6842,   514],\n",
      "        [   13,   198,   198,  ...,  1000,    13,   198]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "iter= train_chunked_tokens.iterator()\n",
    "\n",
    "iteable= iter.iter_torch_batches(batch_size =2)\n",
    "\n",
    "print(next(iteable.iterator_gen()))\n",
    "\n",
    "print(next(iteable.iterator_gen()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.text import Perplexity\n",
    "\n",
    "import ray\n",
    "import ray.train\n",
    "\n",
    "from model.GPT import GPT\n",
    "from utility import save_checkpoint, resume_checkpoint\n",
    "from text_generator import TextGenerator\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    dimension_embedding = config[\"dimension_embedding\"]\n",
    "    block_size = config[\"block_size\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    num_headers = config[\"num_headers\"]\n",
    "    drop_rate = config[\"drop_rate\"]\n",
    "    qkv_bias = config[\"qkv_bias\"]\n",
    "    check_frequency = config[\"check_frequency\"]\n",
    "    batch_size_per_worker = config[\"batch_size_per_worker\"]\n",
    "    num_epoch_per_worker = config[\"num_epoch_per_worker\"]\n",
    "    resume_training = config[\"resume_training\"]\n",
    "    best_checkpoint_dir = config[\"best_checkpoint_dir\"]\n",
    "    start_context = config[\"start_context\"]\n",
    "\n",
    "    # GPT model\n",
    "    model = GPT(\n",
    "        vocab_size,\n",
    "        dimension_embedding,\n",
    "        block_size,\n",
    "        num_layers,\n",
    "        num_headers,\n",
    "        drop_rate,\n",
    "        qkv_bias,\n",
    "    )\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "    # ====== Resume training state from the checkpoint. ======\n",
    "    epoch_start = 0\n",
    "    best_perplexity = 1000000.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    if resume_training:\n",
    "        if os.path.exists(best_checkpoint_dir):\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(best_checkpoint_dir)\n",
    "        else:\n",
    "            checkpoint = None\n",
    "        if checkpoint:\n",
    "            best_epoch, best_perplexity = resume_checkpoint(model, optimizer, checkpoint)\n",
    "            epoch_start = best_epoch\n",
    "            print(\n",
    "                f\"Resumed training from best_epoch {best_epoch},best_perplexity {best_perplexity}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Checkpoint not found, starting from epoch 0\")\n",
    "\n",
    "    # loss function\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    rank = ray.train.get_context().get_world_rank()\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # metrics\n",
    "    metric = Perplexity().to(device)\n",
    "\n",
    "    # data\n",
    "    train_data_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    validate_data_shard = ray.train.get_dataset_shard(\"validate\")\n",
    "\n",
    "    report_metrics = {\n",
    "        \"epoch\": 0,\n",
    "        \"train_loss\": 0.0,\n",
    "        \"validate_loss\": 0.0,\n",
    "        \"perplexity\": 0.0,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_perplexity\": best_perplexity,\n",
    "    }\n",
    " \n",
    " \n",
    "    text_generator = TextGenerator(model,device=device)\n",
    "    tokenizer = TikTokenizer()\n",
    "    for epoch in range(epoch_start + 1, num_epoch_per_worker + 1):\n",
    "        \n",
    "        print(f\"epoch:{epoch}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        report_metrics[\"epoch\"] = epoch\n",
    "\n",
    "        train_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch in train_data_shard.iter_torch_batches(\n",
    "            batch_size=batch_size_per_worker,\n",
    "            drop_last=True,\n",
    "            local_shuffle_buffer_size=1000,\n",
    "        ):\n",
    "            \n",
    "            batch_count += 1\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            target_ids = batch[\"target_ids\"]\n",
    "            loss = loss_function(logits.flatten(0, 1), target_ids.flatten())\n",
    "            train_loss += loss.item()  # only for reporting\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / batch_count\n",
    "\n",
    "        report_metrics[\"train_loss\"] = train_loss\n",
    "\n",
    "        validate_loss = 0\n",
    "        perplexity = 0\n",
    "\n",
    "        if epoch % check_frequency == 0:\n",
    "\n",
    "            model.eval()\n",
    " \n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_count = 0\n",
    "                for batch in validate_data_shard.iter_torch_batches(\n",
    "                    batch_size=1,\n",
    "                    drop_last=False,\n",
    "                ):\n",
    "                    batch_count += 1\n",
    "                    input_ids = batch[\"input_ids\"]\n",
    "                    logits = model(input_ids)\n",
    "                    target_ids = batch[\"target_ids\"]\n",
    "                    loss = loss_function(logits.flatten(0, 1), target_ids.flatten())\n",
    "                    validate_loss += loss.item()  # only for reporting\n",
    "                    metric.update(logits, target_ids)\n",
    "\n",
    "            validate_loss = validate_loss / batch_count\n",
    "            perplexity = metric.compute().item()\n",
    "            metric.reset()\n",
    "\n",
    "            report_metrics[\"validate_loss\"] = validate_loss\n",
    "            report_metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "\n",
    "            # In standard DDP training, where the model is the same across all ranks,\n",
    "            # only the global rank 0 worker needs to save and report the checkpoint\n",
    "            if ray.train.get_context().get_world_rank() == 0:\n",
    "                if perplexity < best_perplexity:\n",
    "                    best_perplexity = perplexity\n",
    "                    best_epoch = epoch\n",
    "                    \n",
    "                    report_metrics[\"best_epoch\"] = best_epoch\n",
    "                    report_metrics[\"best_perplexity\"] = best_perplexity\n",
    "\n",
    "\n",
    "                    # create the best_checkpoint_dir if it does not exist\n",
    "                    # if not os.path.exists(best_checkpoint_dir):\n",
    "                    #     os.makedirs(best_checkpoint_dir)\n",
    "\n",
    "                    # save_checkpoint(\n",
    "                    #     model,\n",
    "                    #     optimizer,\n",
    "                    #     epoch,\n",
    "                    #     perplexity,\n",
    "                    #     best_checkpoint_dir,\n",
    "                    # )\n",
    "\n",
    "            ray.train.report(metrics=report_metrics)\n",
    "                \n",
    "        decoded = text_generator(tokenizer.encode(start_context), max_new_tokens=50, context_size=block_size)\n",
    "        print(f\"\\n epoch{epoch}:{decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import Result\n",
    "from ray import train\n",
    "\n",
    "train_loop_config = {\n",
    "    \"vocab_size\": cfg[\"124M\"][\"vocab_size\"],\n",
    "    \"dimension_embedding\": cfg[\"124M\"][\"dimension_embedding\"],\n",
    "    \"block_size\": cfg[\"124M\"][\"block_size\"],\n",
    "    \"num_layers\": cfg[\"124M\"][\"num_layers\"],\n",
    "    \"num_headers\": cfg[\"124M\"][\"num_headers\"],\n",
    "    \"drop_rate\": cfg[\"124M\"][\"drop_rate\"],\n",
    "    \"qkv_bias\": cfg[\"124M\"][\"qkv_bias\"],\n",
    "    \"check_frequency\": cfg[\"ray_train\"][\"check_frequency\"],\n",
    "    \"batch_size_per_worker\": cfg[\"ray_train\"][\"batch_size_per_worker\"],\n",
    "    \"num_epoch_per_worker\": cfg[\"ray_train\"][\"num_epoch_per_worker\"],\n",
    "    \"resume_training\":cfg[\"ray_train\"][\"resume_training\"],\n",
    "    \"best_checkpoint_dir\":cfg[\"ray_train\"][\"best_checkpoint_dir\"],\n",
    "    \"start_context\":cfg[\"ray_train\"][\"start_context\"],\n",
    "}\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    datasets={\n",
    "        \"train\": train_chunked_tokens,\n",
    "        \"validate\": validate_chunked_tokens,\n",
    "    },\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=cfg[\"ray_train\"][\"num_workers\"],\n",
    "        use_gpu=cfg[\"ray_train\"][\"use_gpu\"],\n",
    "        resources_per_worker={\n",
    "            \"CPU\": cfg[\"ray_train\"][\"num_cpus_per_worker\"],\n",
    "            \"GPU\": cfg[\"ray_train\"][\"num_gpus_per_worker\"],\n",
    "        },\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        storage_path=cfg[\"ray_train\"][\"storage_path\"],\n",
    "        name=cfg[\"ray_train\"][\"name\"],\n",
    "    ),\n",
    "    \n",
    ")\n",
    "result: Result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
