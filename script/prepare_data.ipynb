{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "import sys\n",
    "# Add the directory to the Python path\n",
    "sys.path.append(f\"{project_root}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import ray\n",
    "\n",
    "from config import gpt2_cfg as cfg \n",
    "from datasource_processor import DatasourceProcessor\n",
    "from text_split_processor import TextSplitProcessor\n",
    "from chunk_processor import ChunkProcessor\n",
    "from token_processor import TokenProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = cfg[\"text_dataset\"]\n",
    "\n",
    "\n",
    "data_sources = [Path(item[\"path\"]) for item in text_dataset[\"source\"]]\n",
    "document_paths = ray.data.from_items(data_sources)\n",
    "texts = document_paths.map(DatasourceProcessor())\n",
    "\n",
    "train_ratio = cfg[\"ray_data\"][\"train_ratio\"]\n",
    "text_split_processor = TextSplitProcessor(train_ratio=train_ratio)\n",
    "texts = texts.map(text_split_processor)\n",
    "\n",
    "tokenizer_class = TokenProcessor.create(cfg['ray_data']['tokenizer_class']['name'])\n",
    "tokenizer_args =  cfg['ray_data']['tokenizer_class']['args']\n",
    "tokenizer= tokenizer_class(**tokenizer_args)\n",
    "tokens = texts.map(tokenizer)\n",
    "\n",
    "block_size = cfg[\"model\"][\"block_size\"]\n",
    "stride = cfg[\"model\"][\"stride\"]\n",
    "chunk_processor = ChunkProcessor(block_size=block_size, stride=stride)\n",
    "chunked_tokens = tokens.map(chunk_processor)\n",
    "\n",
    "chunked_tokens.write_parquet(cfg[\"text_dataset\"][\"processed\"])\n",
    "\n",
    "# train_chunked_tokens = []\n",
    "# validate_chunked_tokens = []\n",
    "\n",
    "# for item in chunked_tokens.iter_rows():\n",
    "#     train_chunked_tokens.extend(item[\"train\"])\n",
    "#     validate_chunked_tokens.extend(item[\"validate\"])\n",
    "\n",
    "#self.train_chunked_tokens = ray.data.from_items(train_chunked_tokens)\n",
    "#self.validate_chunked_tokens = ray.data.from_items(validate_chunked_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
