project_root:

output_dir: ${project_root}/outputs
model_weights_dir: ${project_root}/model_weights/shakespeare

dataset:
  name: "tiny_shakespeare"
  path: ${project_root}/dataset/shakespeare_char/shards
  chunked_tokens: ${project_root}/dataset/shakespeare_char/chunked_tokens



ray_debug: "0"   # "0": no debug, "1": debug

model:
  vocab_size: 65
  block_size: 256 # maximum length of the text to be generated
  stride: 256 #  window stride for the dataset.
  dimension_embedding: 384 # embedding dimension
  num_layers: 6 # number of layers
  num_headers: 6 # number of heads
  drop_rate: 0.0 # dropout rate
  bias: False # whether to use bias in attention layer

  openai_gpt_dir: ${model_weights_dir}/openai

ray_data:
  train_ratio: 0.8
  # Tokenizer
  tokenizer_class:
    name: CharTokenizer
    args:
      unique_chars: ["\n", ' ', '!', '$', '&', "'", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
        

ray_train:
  num_workers: 1
  use_gpu: True
  num_gpus_per_worker: 1
  num_cpus_per_worker: 1

  resume_training: False
  check_frequency: 4

  gradient_accumulation_steps: 1
  physical_training_batch_size_per_worker: 64
  physical_validate_batch_size_per_worker: 64
  

  # Training output
  storage_path: ${output_dir}/gpt2
  name: "shakespeare"

  # checkpoint dir
  best_checkpoint_dir: ${model_weights_dir}/best_checkpoint
  latest_checkpoint_dir: ${model_weights_dir}/latest_checkpoint

  start_context: "To be, or not to be: that is the question."


  # Optimizer
  weight_decay: 0.1

  # Scheduler
  warmup_steps: 100
  max_steps: 2000
  max_lr: 1e-3
  min_lr: 6e-5
  beta1: 0.9
  beta2: 0.99
  decay_lr: True

  # Floating point precision
  data_type: bfloat16
