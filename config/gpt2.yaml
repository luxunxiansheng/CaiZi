project_root:

output_dir: ${project_root}/outputs

dataset:
  # - name: "xjtu_1"
  #   path: "/workspaces/CaiZi/dataset/xjtu_1.txt"

  # - name: "qi_wang"
  #   path: "/workspaces/CaiZi/dataset/qi_wang.txt"

  # - name: "shu_wang"
  #   path: "/workspaces/CaiZi/dataset/shu_wang.txt"

  # - name: "hai_zi_wang"
  #   path: "/workspaces/CaiZi/dataset/hai_zi_wang.txt"

  - name: "the_verdict"
    path: "/workspaces/CaiZi/dataset/the_verdict.txt"

train_ratio: 0.9

124M:
  vocab_size: 50257 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  block_size: 1024 # maximum length of the text to be generated
  stride: 1 #  window stride for the dataset. The default value is 1, which means that the dataset is a contiguous block of text.
  n_embd: 768 # embedding dimension
  n_layer: 12 # number of layers
  n_head: 12 # number of heads
  dropout: 0.1 # dropout rate
  bias: False # whether to use bias in attention layer
  

ray_train:
  num_workers: 1
  use_gpu: True
  num_gpus_per_worker: 1
  num_cpus_per_worker: 1

  check_frquency: 1000
  batch_size_per_worker: 4
  num_epoch_per_worker: 1

  
  
  
  

